<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta name="renderer" content="webkit">
        <title>Optional Task - Video Object Detection - ACM MM'21 VRU Challenge</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="ACM Multimedia'21 Grand Challenge">

        <link href="styles/bootstrap.min.css" rel="stylesheet">
        <link href="styles/common.css" rel="stylesheet">
        <link href="styles/show-json.css" rel="stylesheet">
    </head>

    <body data-spy="scroll" data-target=".navbar">
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <h2 class="navbar-brand hidden-xs hidden-sm" style="padding-top: 0px;padding-bottom: 0px;height: 30px;line-height: 35px">
                        <span class="logo">Video Relation Understanding</span>
                    </h2>
                    <h5 class="hidden-xs hidden-sm" style="margin-bottom: 0px">
                        <span style="color:#003d7c !important;">ACM Multimedia 2021 Grand Challenge</span>
                    </h5>
                    <h2 class="navbar-brand hidden-md hidden-lg">
                        <span class="logo">VRU Challenge</span>
                    </h2>
                </div>

                <div class="collapse navbar-collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li><a class="page-scroll" href="../index.html">Overview</a></li>
                        <li><a class="page-scroll" href="../index.html#participation">Participation</a></li>
                        <li><a class="page-scroll" href="../index.html#timeline">Timeline</a></li>
                        <li><a class="page-scroll" href="../index.html#contact">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- linked this part-->
        <section id="introduction" class="scrollable-section">
            <div class="container">
                <h3>Optional Task: Video Object Detection</h3>
                <p>
                    Object detection is the first step towards relation understanding in videos.
                    This task requires participants to develop robust object detectors
                        that not only localize objects of certain categories with bounding boxes in every video frame
                        but also link the bounding boxes that indicate same object entity into a trajectory.
                    This will help machine to understand the identities and dynamics of object entities at video level,
                        which can benefit many applications that require fine-grained video understanding.
                    Furthermore, we hope it can accelerate the research on robust video object detection in the wild,
                        by providing a larger number of user generated videos with annotations.
                    An expectable challenge of this task is that, the detectors should 
                    be able to re-identify an object which disappears sometime in a video. 
                    Technically, the detectors have to overcome difficulties from video quality (e.g. free camera motion, 
                    illumination and blurring) and object variation (e.g. occlusion and deformable shape).
                </p>
            </div>
        </section>
        <section id="dataset" class="scrollable-section">
            <div class="container">
                <h3>Dataset</h3>
                <p>
                    The VidOR dataset for this task consists of user-generated videos from Flickr, 
                    along with annotations on 80 categories of object (e.g., "adult", "child", "dog", "table"). Bounding boxes are annotated
                    for objects in each frame, and the objects' identities among frames are also provided. The training/validation/testing splits
                    are 7,000, 835 and 2,165 videos. Specially, if the object just shows part of its body in the image (e.g., a hand of a person),
                    it is also annotated in this dataset.
                </p>
                <p>
                    The videos and annotations of the training and validation sets can be downloaded directly from 
                    <a href="https://xdshang.github.io/docs/vidor.html#downloads" target="_blank">here</a>.
                    Note that the downloaded annotations contain additional annotation of relations.
                    This task allows participants to use them to train the object detection models.
                </p>
            </div>
        </section>
        <section id="evaluation" class="scrollable-section">
            <div class="container">
                <h3>Evaluation Metric</h3>
                <p>
                    We adopt average precision (AP) as metric to evaluate the detection performance for each object category. The trajectory-level 
                    mean AP (mAP) is defined as follows:
                </p>
                <p>
                    Given a predicted trajectory (a.k.a. tubelet) $\mathcal{T}_p$ and a ground truth 
                    trajectory $\mathcal{T}_g$ of a certain category, the temporal Intersection over Union (tIoU) between these two trajectories is calculated as:
                        $$\text{tIoU}(\mathcal{T}_p, \mathcal{T}_g)=\frac{D_p \cap D_g}{D_p\cup D_g}, $$
                    where $D_p$, $D_g$ denote the time duration of the predicted trajectory and the ground truth trajectory respectively. In out settings, the threshold for \( \text{tIoU} \) is 0.5,
                    which means any result with \( \text{tIoU} \geq 0.5 \) will be regarded as true positive prediction. Besides, the \( \text{IoU} \) threshold for frame-level bounding box is set to (0.5, 0.7, 0.9). For each trajectory pair,
                    their \( \text{tIoU} \) is averaged on the three frame-level IoU values. 
                </p>
                <p>
                    We only consider the <b>top-20 predictions</b> for each video. The final mAP is obtained by averaging the APs across all the object categories.
                </p>
                <p>
                    The evaluation code used by the evaluation server can be found
                    <a href="https://github.com/NExTplusplus/VidVRD-helper/blob/master/evaluation/video_object_detection.py" target="_blank">here</a>.
                </p>
            </div>
        </section>
        <section id="submission" class="scrollable-section">
            <div class="container">
                <h3>Submission Format</h3>
                <p>
                    Please use the following JSON format to submit your results to the submission server.
                    Before uploading the result, please compress the JSON file using
                    <a href="https://github.com/NExTplusplus/VidVRD-helper/blob/master/evaluation/README.md">XZ</a>
                    and make sure the final file size not exceed 100MB.
                </p>
                <pre id="sample"></pre>
                <p>
                    The example above is illustrative. Comments must be removed in your submission.
                    A sample submission file is available <a href="assets/submission-task2.json">here</a>.
                </p>
            </div>
        </section>

        <footer class="footer" style="padding-top: 50px;">
            <div class="container">
                <hr>
                <p style="font-style: italic; text-align: center">
                    Copyright &copy; 2018-2020 NExT++ /
                    <a class="black" href="http://www.nextcenter.org/privacy-policy">Privacy Policy</a> /
                    <a class="black" href="http://www.nextcenter.org/terms-conditions">Terms &amp; Conditions</a> /
                    <a class="black" href="faq.html">FAQs</a>
                </p>
            </div>
        </footer>

        <script type="text/javascript" src="scripts/jquery-3.2.1.min.js"></script>
        <script type="text/javascript" src="scripts/bootstrap.min.js"></script>
        <script type="text/javascript" src="scripts/jquery.easing.min.js"></script>
        <script type="text/javascript" src="scripts/scrolling-nav.js"></script>
        <script type="text/javascript" src="scripts/show-json.js"></script>
        <script type="text/javascript">
            const sample = "{\n" +
            "  version: \"VERSION 1.0\",\n" +
            "  results: {\n" +
            "    \"8173050335\": [                             # video ID\n" +
            "      {                                         # a trajectory instance\n" +
            "        category: \"dog\",                        # trajectory category\n" +
            "        score: 0.8943,                          # trajectory score float \n" +
            "        trajectory: {                           # trajectory\n" +
            "          \"5\": [9, 10, 45, 20],                 # frame_id: [x_min, y_min, x_max, y_max] int (all inclusive) \n" +
            "          \"6\": [10, 17, 46, 22],                   \n" +
            "          \"17\": [11, 12, 40, 25],\n" +
            "          \"19\": [13, 12, 43, 27]\n" +
            "        },\n" +
            "      },\n" +
            "      {                                         # another trajectory instance\n" +
            "        category: \"adult\", \n" +
            "        score: 0.6626,\n" +
            "        trajectory: { \n" +
            "         \"0\": [89, 10, 128, 80], \n" +
            "         \"1\": [90, 10, 128, 79]\n" +
            "        },\n" +
            "      }\n" +
            "    ]\n" +
            "  }\n" +
            "  external_data: {\n" +
            "    used: true,                                 # Boolean flag. True indicates the use of external data.\n" +
            "    details: \"First fully-connected layer from VGG-16 pre-trained on ILSVRC-2012 training set\" # This string details what kind of external data you used and how you used it.\n" +
            "  }\n" +
            "}";
            $('#sample').html(syntaxHighlight(sample));
        </script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
        </script>
        <script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
        
    </body>
</html>
